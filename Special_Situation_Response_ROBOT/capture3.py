import os
os.environ["QT_LOGGING_RULES"] = "qt.qpa.*=false"

import sys
import cv2
import numpy as np
import torch
import time
from datetime import datetime
from pathlib import Path
import threading
import requests
import base64
import json
import select
import socket
# ìŒì„± ì¸ì‹ìš©
import pyaudio
import wave
import speech_recognition as sr
import tempfile
import google.generativeai as genai

# í™˜ê²½ ë³€ìˆ˜ ë° ì»¤ë„ ì„¤ì •
os.environ["TORCH_CUDNN_SDPA_ENABLED"] = "1"
torch.backends.cuda.enable_flash_sdp = False
torch.backends.cuda.enable_mem_efficient_sdp = True
torch.backends.cuda.enable_math_sdp = True

# SAM2 ê´€ë ¨ ì„í¬íŠ¸
sys.path.append('/home/j/sam4/src/SAM2_streaming')
from sam2.build_sam import build_sam2_camera_predictor

# RAFT-Stereo ê´€ë ¨ ì„í¬íŠ¸ ë° ê²½ë¡œ ì¶”ê°€
sys.path.append('core')
from raft_stereo import RAFTStereo
from utils.utils import InputPadder

# ====== ì‚¬ìš©ì íŒŒë¼ë¯¸í„°/ê²½ë¡œ ======
calib_data = np.load('/home/j/stereo/stereo_calibration_data.npz')
mtx_left = calib_data['mtx_left']
dist_left = calib_data['dist_left']
mtx_right = calib_data['mtx_right']
dist_right = calib_data['dist_right']
R = calib_data['R']
T = calib_data['T']

model_version = 'sam2.1'
model_size = 'tiny'
sam2_checkpoint = f"./checkpoints/{model_version}/{model_version}_hiera_{model_size}.pt"
model_cfg = f"{model_version}/{model_version}_hiera_{model_size[0]}.yaml"

DEVICE = 'cuda'
torch.cuda.empty_cache()

# RAFT-Stereo ì„¤ì •
raft_args = type('', (), {})()
raft_args.restore_ckpt = 'models/raftstereo-middlebury.pth'
raft_args.output_directory = 'demo_output/example/test12'
raft_args.save_numpy = False
raft_args.mixed_precision = False
raft_args.valid_iters = 128  # ì •ë°€ ê³„ì‚°ìš© ë°˜ë³µ íšŸìˆ˜ (ë†’ìŒ)
raft_args.hidden_dims = [128]*3
raft_args.corr_implementation = 'reg_cuda'
raft_args.shared_backbone = False
raft_args.corr_levels = 4
raft_args.corr_radius = 4
raft_args.n_downsample = 2
raft_args.context_norm = 'instance'
raft_args.slow_fast_gru = True
raft_args.n_gru_layers = 3

# V4L2 ë°±ì—”ë“œ ì‚¬ìš©
cap_left = cv2.VideoCapture(2, cv2.CAP_V4L2)
cap_right = cv2.VideoCapture(4, cv2.CAP_V4L2)
cap_left.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
cap_right.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
cap_left.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap_left.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
cap_right.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap_right.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
cap_left.set(cv2.CAP_PROP_FPS, 30)
cap_right.set(cv2.CAP_PROP_FPS, 30)

ret_left, frame_left = cap_left.read()
ret_right, frame_right = cap_right.read()

print(f"ì™¼ìª½ í•´ìƒë„: {cap_left.get(cv2.CAP_PROP_FRAME_WIDTH)} x {cap_left.get(cv2.CAP_PROP_FRAME_HEIGHT)}")

image_size = (frame_left.shape[1], frame_left.shape[0])
R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(
    mtx_left, dist_left, mtx_right, dist_right,
    image_size, R.T, -T, alpha=-1, flags=cv2.CALIB_ZERO_DISPARITY
)
left_mapx, left_mapy = cv2.initUndistortRectifyMap(
    mtx_left, dist_left, R1, P1, image_size, cv2.CV_32FC1)
right_mapx, right_mapy = cv2.initUndistortRectifyMap(
    mtx_right, dist_right, R2, P2, image_size, cv2.CV_32FC1)

save_dir = raft_args.output_directory
os.makedirs(save_dir, exist_ok=True)

# --- [ìµœì í™”] NumPy ì´ë¯¸ì§€ë¥¼ ë°”ë¡œ GPU Tensorë¡œ ë³€í™˜ ---
def frame_to_tensor(frame):
    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    img = torch.from_numpy(img).permute(2, 0, 1).float()
    return img[None].to(DEVICE)

# --- ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ---
def load_raftstereo_model(raft_args, device):
    model = RAFTStereo(raft_args).to(device)
    
    if torch.cuda.is_available():
        checkpoint = torch.load(raft_args.restore_ckpt, map_location=torch.device('cuda'), weights_only=False)
    else:
        checkpoint = torch.load(raft_args.restore_ckpt, map_location=torch.device('cpu'), weights_only=False)
        
    new_checkpoint = {k.replace('module.', ''): v for k, v in checkpoint.items()}
    
    filtered_checkpoint = {}
    for k, v in new_checkpoint.items():
        if 'running_mean' in k or 'running_var' in k:
            continue
        filtered_checkpoint[k] = v
        
    model.load_state_dict(filtered_checkpoint, strict=False)
    model.eval()
    return model

raftstereo_model = load_raftstereo_model(raft_args, DEVICE)

# --- [í•µì‹¬] ì‹¤ì‹œê°„ ê¹Šì´ ë§µ ìƒì„± í•¨ìˆ˜ (ì†ë„ ìµœì í™”) ---
def get_realtime_disparity(left_img, right_img, model=raftstereo_model, viz_iters=12):
    """
    ì‹¤ì‹œê°„ ì‹œê°í™”ë¥¼ ìœ„í•´ ì ì€ ë°˜ë³µ íšŸìˆ˜(viz_iters)ë¡œ disparityë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    with torch.no_grad():
        image1 = frame_to_tensor(left_img)
        image2 = frame_to_tensor(right_img)

        padder = InputPadder(image1.shape, divis_by=32)
        image1_pad, image2_pad = padder.pad(image1, image2)
        
        # test_mode=True, itersë¥¼ ë‚®ê²Œ ì„¤ì •í•˜ì—¬ ê³ ì† ì¶”ë¡ 
        _, flow_up = model(image1_pad, image2_pad, iters=viz_iters, test_mode=True)
        
        disparity = -padder.unpad(flow_up).squeeze().cpu().numpy()
        
        # ì‹œê°í™”ë¥¼ ìœ„í•œ ì •ê·œí™” (0~255)
        disp_vis = cv2.normalize(disparity, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
        disp_vis = np.uint8(disp_vis)
        disp_color = cv2.applyColorMap(disp_vis, cv2.COLORMAP_MAGMA)
        
        return disp_color

# --- ì •ë°€ ì¢Œí‘œ ê³„ì‚° í•¨ìˆ˜ ---
def calculate_centroid_optimized(left_img, right_img, mask_img, model=raftstereo_model):
    start = time.time()
    Q_flipped = Q.copy()
    Q_flipped[3,2] = -Q_flipped[3,2]

    with torch.no_grad():
        image1 = frame_to_tensor(left_img)
        image2 = frame_to_tensor(right_img)

        padder = InputPadder(image1.shape, divis_by=32)
        image1_pad, image2_pad = padder.pad(image1, image2)
        
        # ì •ë°€ ê³„ì‚° (iters=128)
        
        _, flow_up = model(image1_pad, image2_pad, iters=raft_args.valid_iters, test_mode=True)
        
        disparity = -padder.unpad(flow_up).squeeze().cpu().numpy()
        h, w = disparity.shape
        
        points_3d = cv2.reprojectImageTo3D(disparity.astype(np.float32), Q_flipped, handleMissingValues=True)
        
        # ìœ íš¨ì„± í•„í„°ë§
        z_valid = (points_3d[..., 2] > 0) & (points_3d[..., 2] < 5000)
        valid_mask = (disparity > 0.1) & np.isfinite(disparity)
        total_mask = valid_mask & z_valid
        
        if mask_img is not None:
            mask_resized = cv2.resize(mask_img, (w, h), interpolation=cv2.INTER_NEAREST)
            total_mask = total_mask & (mask_resized > 0)
            
        valid_points = points_3d[total_mask]
        
        centroid = None
        if len(valid_points) > 0:
            centroid = np.mean(valid_points, axis=0)
            
    end = time.time()
    return centroid, end - start, len(valid_points)

# Google Cloud Vision API
API_KEY = 'AIzaSyBtTjPi2jBhiuhXNoIt_Ay3FPx_rQRMvrU'
VISION_ENDPOINT = f'https://vision.googleapis.com/v1/images:annotate?key={API_KEY}'

def analyze_object_localization(image_path):
    try:
        with open(image_path, 'rb') as img_file:
            content = base64.b64encode(img_file.read()).decode('utf-8')
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return None

    payload = {
        "requests": [{
            "image": {"content": content},
            "features": [{"type": "OBJECT_LOCALIZATION", "maxResults": 10}]
        }]
    }
    
    try:
        start_time = time.time()
        response = requests.post(VISION_ENDPOINT, json=payload)
        elapsed = time.time() - start_time
        print(f"â±ï¸ Vision API ì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ")

        if response.ok:
            return response.json()
        else:
            print(f"âŒ Vision API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        print(f"âŒ API ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None

genai.configure(api_key="AIzaSyD1mxECQtGPV8XyP-Ypbq2y0D7sXe8dPbk")

def match_object_with_gemini(voice_command, detected_objects):
    objects_text = "\n".join([
        f"{i+1}. {obj['name']} (ì‹ ë¢°ë„: {obj.get('score', 0):.2f})"
        for i, obj in enumerate(detected_objects)
    ])
    prompt = (
        "ë‹¤ìŒì€ ì´ë¯¸ì§€ì—ì„œ ê°ì§€ëœ ê°ì²´ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
        f"{objects_text}\n\n"
        f"ì‚¬ìš©ìì˜ ìŒì„± ëª…ë ¹: \"{voice_command}\"\n\n"
        "ëª©ë¡ì— í•´ë‹¹ ê°ì²´ê°€ ì—†ìœ¼ë©´ 'ì—†ìŒ'ì´ë¼ê³  ëŒ€ë‹µí•˜ê³ , "
        "ìˆìœ¼ë©´ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ê°ì²´ì˜ ë²ˆí˜¸ë§Œ ìˆ«ìë¡œ ëŒ€ë‹µí•˜ì„¸ìš”."
    )

    try:
        start_time = time.time()
        model = genai.GenerativeModel('gemini-2.0-flash')
        response = model.generate_content(prompt)
        elapsed = time.time() - start_time
        print(f"â±ï¸ Gemini ì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ")

        text = response.text.strip()
        if 'ì—†ìŒ' in text or 'ì—†ë‹¤ê³ ' in text:
            print("âŒ Gemini: í•´ë‹¹ ê°ì²´ê°€ ëª©ë¡ì— ì—†ìŠµë‹ˆë‹¤.")
            return None

        import re
        nums = re.findall(r'\d+', text)
        if nums:
            idx = int(nums[0]) - 1
            if 0 <= idx < len(detected_objects):
                return detected_objects[idx]
        print("âŒ Gemini ì‘ë‹µì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ Gemini API ì˜¤ë¥˜: {e}")
        return None

# ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ í´ë˜ìŠ¤
class VoiceRecognitionSystem:
    def __init__(self, device_index=None):
        print("ğŸš€ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.p = pyaudio.PyAudio()
        self.device_index = device_index
        self.recognizer = sr.Recognizer()
        self.rate = self.get_supported_sample_rate()
        print("ğŸ¯ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")

    def get_supported_sample_rate(self):
        sample_rates = [44100, 48000, 22050, 16000, 8000]
        for rate in sample_rates:
            try:
                stream = self.p.open(format=self.format, channels=self.channels, rate=rate,
                                   input=True, frames_per_buffer=self.chunk, input_device_index=self.device_index)
                stream.close()
                return rate
            except: continue
        return 44100

    def print_device_list(self):
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ ë¦¬ìŠ¤íŠ¸:")
        input_devices = []
        for i in range(self.p.get_device_count()):
            info = self.p.get_device_info_by_index(i)
            if info['maxInputChannels'] > 0:
                input_devices.append((i, info))
                print(f"{i}: {info['name']}")
        return input_devices

    def select_device_interactive(self):
        devices = self.print_device_list()
        if not devices: return None
        while True:
            try:
                choice = input(f"\nì‚¬ìš©í•  ì…ë ¥ ì¥ì¹˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”: ")
                return int(choice)
            except: return None

    def record_audio_blocking(self):
        try:
            stream = self.p.open(format=self.format, channels=self.channels, rate=self.rate,
                                 input=True, frames_per_buffer=self.chunk, input_device_index=self.device_index)
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì¥ì¹˜ ì˜¤ë¥˜: {e}")
            return None, 0
            
        frames = []
        print("ğŸ¤ Enter í‚¤ë¥¼ ëˆŒëŸ¬ ë…¹ìŒ ì‹œì‘...")
        input()
        print("ğŸ”´ ë…¹ìŒ ì‹œì‘! (ì¢…ë£Œí•˜ë ¤ë©´ ë‹¤ì‹œ Enter)")
        
        while True:
            if select.select([sys.stdin], [], [], 0)[0]:
                sys.stdin.readline()
                break
            data = stream.read(self.chunk, exception_on_overflow=False)
            frames.append(data)
            
        stream.stop_stream()
        stream.close()
        
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            filename = tmp_file.name
            wf = wave.open(filename, 'wb')
            wf.setnchannels(self.channels)
            wf.setsampwidth(self.p.get_sample_size(self.format))
            wf.setframerate(self.rate)
            wf.writeframes(b''.join(frames))
            wf.close()
        return filename, 0

    def transcribe(self, wav_path, record_duration):
        if not wav_path: return None
        print("ğŸ”„ ìŒì„± ì¸ì‹ ì¤‘...")
        try:
            with sr.AudioFile(wav_path) as source:
                audio_data = self.recognizer.record(source)
            text = self.recognizer.recognize_google(audio_data, language='ko-KR')
            print(f"ğŸ—£ï¸ ì¸ì‹ ê²°ê³¼: '{text}'")
            return text
        except sr.UnknownValueError:
            print("â“ ìŒì„±ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        except sr.RequestError as e:
            print(f"âŒ êµ¬ê¸€ ìŒì„± API ì˜¤ë¥˜: {e}")
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

    def run(self):
        global voice_command, running
        try:
            while True:
                wav_path, duration = self.record_audio_blocking()
                if wav_path:
                    result = self.transcribe(wav_path, duration)
                    try: os.unlink(wav_path)
                    except: pass
                    
                    if result:
                        voice_command = result.lower()
                        print(f"ğŸ”Š ëª…ë ¹ì–´: {voice_command}")
                        parse_and_trigger(voice_command)
        except: pass
        finally: self.p.terminate()

def parse_and_trigger(cmd):
    global process_flag, analyze_flag, current_voice_command, tracking_active
    if 'ë¶„ì„' in cmd or 'analyze' in cmd:
        analyze_flag = True
        current_voice_command = cmd
        print("âœ… ë¶„ì„ ëª¨ë“œ í™œì„±í™”")
    elif 'ì €ì¥' in cmd or 'save' in cmd:
        process_flag = True
    elif 'ì‹œì‘' in cmd or 'start' in cmd or 'ì¡ì•„' in cmd: # <--- [ì¶”ê°€] ìŒì„± ëª…ë ¹ ì˜ˆì‹œ
        tracking_active = True
        print("ğŸš€ [ëª…ë ¹ ìˆ˜ì‹ ] ë¡œë´‡íŒ” ì¶”ì /ì´ë™ ì‹œì‘!")

# ê¸€ë¡œë²Œ ë³€ìˆ˜
point = None
point_selected = False
process_flag = False
analyze_flag = False
tracking_active = False
auto_process_after_selection = False
frames_after_selection = 0
current_voice_command = ""
if_init = False      # <--- [í•„ìˆ˜ ì¶”ê°€] ì´ˆê¸°í™” ë³€ìˆ˜
rect_left = None

DISPLAY_SIZE = (720, 540)
with torch.autocast(device_type="cuda", dtype=torch.float16):
    predictor = build_sam2_camera_predictor(model_cfg, sam2_checkpoint)

voice_command = ''
running = True
voice_system = VoiceRecognitionSystem(device_index=None)
device_index = 0  # [ìˆ˜ì •ë¨] 0ë²ˆìœ¼ë¡œ ê³ ì •

voice_system.device_index = device_index
threading.Thread(target=voice_system.run, daemon=True).start()

def mouse_callback(event, x, y, flags, param):
    global point, point_selected, if_init, rect_left
    h, w = rect_left.shape[:2]
    actual_x = int(x * w / DISPLAY_SIZE[0])
    actual_y = int(y * h / DISPLAY_SIZE[1])
    
    if event == cv2.EVENT_LBUTTONDOWN:
        point = [actual_x, actual_y]
        point_selected = True
        print(f"ğŸ“ ì„ íƒë¨: ({actual_x}, {actual_y})")
    elif event == cv2.EVENT_RBUTTONDOWN:
        point = None
        point_selected = False
        if_init = False
        print("ğŸ”„ ë¦¬ì…‹")

print("\n=== [ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ] ===")
print("ì¢Œí´ë¦­: ê°ì²´ì„ íƒ | 's'í‚¤: ì¢Œí‘œê³„ì‚° | ìŒì„±: 'ë¶„ì„í•´ì¤˜', 'ì €ì¥í•´'")

# ====== ë©”ì¸ ë£¨í”„ ======
frame_count = 0 # [ìµœì í™”] í”„ë ˆì„ ì¹´ìš´í„° ì¶”ê°€
prev_time = time.time()
while True:
    frame_count += 1
    
    ret_left, frame_left = cap_left.read()
    ret_right, frame_right = cap_right.read()
    if not ret_left or not ret_right: break

    # 1. Rectification
    rect_left = cv2.remap(frame_left, left_mapx, left_mapy, cv2.INTER_LINEAR)
    rect_right = cv2.remap(frame_right, right_mapx, right_mapy, cv2.INTER_LINEAR)
    
    x1, y1, w1, h1 = roi1
    x2, y2, w2, h2 = roi2
    h = min(h1, h2)
    w = min(w1, w2)
    rect_left = rect_left[y1:y1+h, x1:x1+w]
    rect_right = rect_right[y2:y2+h, x2:x2+w]

    try:
        # ì‹œê°í™”ìš© ê¹Šì´ ë§µì€ ë§¤ í”„ë ˆì„ ê·¸ë¦¬ê¸° (viz_iters=10ì´ë¼ ë¹ ë¦„)
        depth_viz = get_realtime_disparity(rect_left, rect_right, viz_iters=10)
        cv2.imshow('Real-time Depth (Preview)', cv2.resize(depth_viz, DISPLAY_SIZE))
    except Exception as e:
        print(f"Depth Viz Error: {e}")

# 2. ìŒì„± ë¶„ì„ ë¡œì§ (API)
    if analyze_flag:
        analyze_flag = False
        timestamp = datetime.now().strftime("%H%M%S")
        temp_path = f"{save_dir}/analyze_{timestamp}.png"
        cv2.imwrite(temp_path, rect_left) 
        
        print(f"ğŸ” Vision API ë¶„ì„ ìš”ì²­...")
        result = analyze_object_localization(temp_path)
        
        found_obj = False
        if result:
            responses = result.get("responses", [])
            if responses:
                objects = responses[0].get("localizedObjectAnnotations", [])
                
                # ==========================================
                # [ì¶”ê°€ë¨] ê°ì§€ëœ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ë””ë²„ê¹…
                # ==========================================
                print(f"\nğŸ“‹ [Vision API ê°ì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸] ì´ {len(objects)}ê°œ ê°ì§€ë¨")
                if len(objects) == 0:
                    print("   ğŸ‘‰ ê°ì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    for i, obj in enumerate(objects):
                        name = obj.get('name', 'Unknown')
                        score = obj.get('score', 0.0)
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì ë„ ê°™ì´ ì°ì–´ë³´ë©´ ì¢‹ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)
                        vertices = obj.get("boundingPoly", {}).get("normalizedVertices", [])
                        cx_info = f"cx:{sum([v.get('x', 0) for v in vertices])/len(vertices):.2f}" if vertices else "N/A"
                        
                        print(f"   ğŸ”¹ {i+1}. {name} (ì‹ ë¢°ë„: {score:.2f}, {cx_info})")
                print("-" * 40 + "\n")
                # ==========================================

                if objects:
                    best_obj = match_object_with_gemini(current_voice_command, objects)
                    if best_obj:
                        vertices = best_obj.get("boundingPoly", {}).get("normalizedVertices", [])
                        if vertices:
                            cx = sum([v.get('x', 0) for v in vertices]) / len(vertices)
                            cy = sum([v.get('y', 0) for v in vertices]) / len(vertices)
                            point = [int(cx * rect_left.shape[1]), int(cy * rect_left.shape[0])]
                            point_selected = True
                            auto_process_after_selection = True
                            frames_after_selection = 0
                            print(f"âœ… ìë™ ì„ íƒë¨: {best_obj.get('name')}")
                            found_obj = True
        
        if not found_obj:
            print("âš ï¸ ì ì ˆí•œ ëŒ€ìƒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 3. SAM2 ë° í™”ë©´ í‘œì‹œ
    show_frame = rect_left.copy()
    mask_img = None
    
    if point_selected:
        with torch.autocast(device_type="cuda", dtype=torch.float16):
            if not globals().get('if_init', False):
                if_init = True
                predictor.load_first_frame(rect_left)
                labels = np.array([1], dtype=np.int32)
                points_arr = np.array([point], dtype=np.float32)
                _, out_obj_ids, out_mask_logits = predictor.add_new_prompt(
                    frame_idx=0, obj_id=(1,), points=points_arr, labels=labels
                )
            else:
                out_obj_ids, out_mask_logits = predictor.track(rect_left)
            
            out_mask = (out_mask_logits[0] > 0.0).permute(1, 2, 0).cpu().numpy().astype(np.uint8)
            mask_img = (out_mask[:, :, 0] * 255).astype(np.uint8)
            
            contours, _ = cv2.findContours(mask_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            show_frame = rect_left.copy()
            cv2.drawContours(show_frame, contours, -1, (0, 165, 255), 3)  # BGR: ì£¼í™©ìƒ‰, ë‘ê»˜ 3
            
            if auto_process_after_selection:
                frames_after_selection += 1
                if frames_after_selection >= 10:
                    print("ğŸ¯ ì•ˆì •í™” ì™„ë£Œ. ì¢Œí‘œ ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                    tracking_active = True
                    auto_process_after_selection = False

    cv2.imshow('Left Camera (Control)', cv2.resize(show_frame, DISPLAY_SIZE))
    cv2.setMouseCallback('Left Camera (Control)', mouse_callback)

    key = cv2.waitKey(1)
    
    # -------------------------------------------------------------------------
    # [ìˆ˜ì •ë¨] ì—°ì† ì¢Œí‘œ ì¶”ì  ë° ì „ì†¡ ë¡œì§ (ì„±ëŠ¥ ìµœì í™” ì ìš©)
    # -------------------------------------------------------------------------
# -------------------------------------------------------------------------
    # [ìˆ˜ì •ë¨] ì—°ì† ì¢Œí‘œ ì¶”ì  ë° ì „ì†¡ ë¡œì§ (íŠ¸ë¦¬ê±° ì¶”ê°€)
    # -------------------------------------------------------------------------
    if point_selected and mask_img is not None:
        
        # 's' í‚¤ë¥¼ ëˆŒëŸ¬ ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if tracking_active:
            if frame_count % 3 == 0: # 3í”„ë ˆì„ë§ˆë‹¤ ì „ì†¡ (ë¶€í•˜ ì¡°ì ˆ)
                
                # ì¢Œí‘œ ê³„ì‚°
                centroid, elapsed, n_pts = calculate_centroid_optimized(rect_left, rect_right, mask_img)
                
                if centroid is not None:
                    cx, cy, cz = centroid
                    
                    # ë°ì´í„° íŒ¨í‚¤ì§•
                    data_dict = {
                        "x": round(float(cx), 2), 
                        "y": round(float(cy), 2), 
                        "z": round(float(cz), 2)
                    }
                    json_data = json.dumps(data_dict)

                    try:
                        # ì†Œì¼“ ì „ì†¡ (ë§¤ë²ˆ ì—´ê³  ë‹«ì•„ ì•ˆì „ì„± í™•ë³´)
                        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                        sock.sendto(json_data.encode(), ("127.0.0.1", 5005))
                        sock.close()
                        
                        # [ì„±ê³µ ë¡œê·¸] ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸
                        print(f"\033[92mğŸ“¡ [ì „ì†¡ ì„±ê³µ] {json_data} (Z={cz:.1f}mm)\033[0m")
                        
                    except Exception as e:
                        # [ì—ëŸ¬ ë¡œê·¸] ë¹¨ê°„ìƒ‰ í…ìŠ¤íŠ¸ (ì´ì œ ì›ì¸ì„ ì•Œ ìˆ˜ ìˆìŒ!)
                        print(f"\033[91mâŒ [ì†Œì¼“ ì—ëŸ¬] {e}\033[0m")
                else:
                    # [ê³„ì‚° ì‹¤íŒ¨] ë…¸ë€ìƒ‰ í…ìŠ¤íŠ¸
                    print(f"\033[93mâš ï¸ [Depth ì‹¤íŒ¨] ê±°ë¦¬ ì¸¡ì • ë¶ˆê°€ (ìœ íš¨ í¬ì¸íŠ¸ ì—†ìŒ)\033[0m")
        else:
            # ì„ íƒì€ ëëŠ”ë° 's'ë¥¼ ì•ˆ ëˆ„ë¥¸ ê²½ìš° (60í”„ë ˆì„ë§ˆë‹¤ ì•Œë¦¼)
            if frame_count % 60 == 0:
                print("â¸ï¸ [ëŒ€ê¸°] ê°ì²´ ì„ íƒë¨. ì „ì†¡í•˜ë ¤ë©´ 's' í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”.")

    # -------------------------------------------------------------------------
    # í‚¤ ì…ë ¥ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±° ë° ì •ë¦¬)
    # -------------------------------------------------------------------------
    if key == ord('s'):
        tracking_active = True
        print("\nğŸš€ [Start] ë°ì´í„° ì „ì†¡ì„ ì‹œì‘í•©ë‹ˆë‹¤!")

    if key == 27: # ESC
        break
    elif key == ord('r'): # ë¦¬ì…‹
        point_selected = False
        if_init = False
        tracking_active = False
        mask_img = None
        print("ğŸ”„ [Reset] ì¶”ì  ì´ˆê¸°í™”")

cap_left.release()
cap_right.release()
cv2.destroyAllWindows()